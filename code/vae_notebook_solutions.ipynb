{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# How to implement a Variational Autoencoder (VAE)\n",
    "\n",
    "A variational autoencoder observes data, infers a latent code for it and tries to reconstruct the data from that latent code. In contrast to regular autoencoders, the code of the VAE is **random**. That means that when presented with the same input, the VAE will produce a slightly different code each time. This makes its decoding process more robust, since it has to deal with noisy code.\n",
    "\n",
    "Another way of looking at a VAE is as a training procedure for a probablistic model. The model is \n",
    "$$p(x) = \\int p(z)p(x|z) dz$$\n",
    "where $z$ is the latent code and $x$ is the data. During training we need to infer a posterior over $z$. In the case of a VAE this is done by neural network.\n",
    "\n",
    "Assuming that the theory of VAEs has already been presented, we now dive straight into implementing them. If you need more background on VAEs, have a look at our [tutorial slides](https://github.com/philschulz/VITutorial/tree/master/modules) and the references therein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Framework\n",
    "\n",
    "For the purpose of this tutorial we are going to use [mxnet](https://mxnet.incubator.apache.org) which is a scalable deep learning library that has interfaces for several languages, including python. We are going to import and abbreviate it as \"mx\". We will use mxnet to define computation graph. This is done using the [symbol library](https://mxnet.incubator.apache.org/api/python/symbol.html). When building the VAE, all the methods that you use should be prefixed with `mx.sym`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import urllib.request\n",
    "import os, logging, sys\n",
    "from os.path import join, exists\n",
    "from abc import ABC\n",
    "from typing import List, Tuple, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify a couple of variables that will help us to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LEARNING_RATE = 0.0003\n",
    "\n",
    "data_names = ['train', 'valid', 'test']\n",
    "train_set = ['train', 'valid']\n",
    "test_set = ['test']\n",
    "data_dir = join(os.curdir, \"binary_mnist\")\n",
    "data_names = ['train', 'valid', 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set up basic logging facilities to print intermediate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s [%(levelname)s]: %(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "Throughout the tutorial we will use the binarised MNIST data set consisting of images of handwritten digits (0-9). Each pixel has been mapped to either 0 or 1, meaning that pixels are either fully on or off. We use this data set because it allows us to use a rather simple product of Bernoullis as a likelihood. We download the data into a folder called \"binary_mnist\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "for data_set in data_names:\n",
    "    file_name = \"binary_mnist.{}\".format(data_set)\n",
    "    goal = join(data_dir, file_name)\n",
    "    if exists(goal):\n",
    "        logging.info(\"Data file {} exists\".format(file_name))\n",
    "    else:\n",
    "        logging.info(\"Downloading {}\".format(file_name))\n",
    "        link = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/binarized_mnist_{}.amat\".format(\n",
    "            data_set)\n",
    "        urllib.request.urlretrieve(link, goal)\n",
    "        logging.info(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now we have the data on disk. We will load it later for training and testing. But first, we need to build our VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagonal Gaussian VAE\n",
    "\n",
    "The most basic VAE model is one where we assume that the latent variable is multiviariate Gaussian. We fix the prior to be standard normal. During inference, we use a multivariate Gaussian variational distribution with diagonal covariance matrix. This means that we are only modelling variance but not covariance (in fact, a k-dimensional Guassian with diagonal covariance has the same density as a product of k independent univariate Gaussians). Geometrically, this variational distribution can only account for spherical but not for eliptical densities. It is thus rather limited in its modelling capabilities. Still, because it uses a neural network under the hood, it is very expressive. \n",
    "\n",
    "In this tutorial, we will model the mist binarised digit data set. Each image is encoded as a 784-dimensional vector. We will model each of these vectors as a product of 784 Bernoullis (of course, there are better models but we want to keep it simple). Our likelihood is thus a product of independent Bernoullis. The resulting model is formally specified as \n",
    "\n",
    "\\begin{align}z \\sim \\mathcal{N}(0,I) && x_i|z \\sim Bernoulli(NN_{\\theta}(z))~~~ i \\in \\{1,2,\\ldots, 784\\} \\ .\\end{align}\n",
    "\n",
    "The variational approximation is given by $$q(z|x) = \\mathcal{N}(NN_{\\lambda}(x), NN_{\\lambda}(x)).$$\n",
    "\n",
    "Notice that both the Bernoulli likelihood and the Gaussian variational distribution use NNs to compute their parameters. The parameters of the NNs, however, are different ($\\theta$ and $\\lambda$, respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will spread our implementation across 3 classes. This design choice is motivated by the desire to make our models as modular as possible. This will later allow us to mix and match different likelihoods and variational distributions.\n",
    "\n",
    "* **Generator**: This class defines our likelihood. Given a latent value, it will can produce a data sample our assign a density to an existing data point.\n",
    "* **InferenceNetwork**: This neural network computes the parameters of the variational approximation from a data point.\n",
    "* **VAE**: This is the variational autoencoder. It combines a Generator and an InferenceNetwork and trains them jointly. Once trained, it can generate random data points or try to reproduce data presented to it.\n",
    "\n",
    "Below we have specified these classes abstractly. Make sure you understand what each method is supposed to be doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(ABC):\n",
    "    \"\"\"\n",
    "    Generator network.\n",
    "\n",
    "    :param data_dims: Dimensionality of the generated data.\n",
    "    :param layer_sizes: Size of each layer in the network.\n",
    "    :param act_type: The activation after each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dims: int, layer_sizes: List[int], act_type: str) -> None:\n",
    "        self.data_dims = data_dims\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.act_type = act_type\n",
    "\n",
    "    def generate_sample(self, latent_state: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Generate a data sample from a latent state.\n",
    "\n",
    "        :param latent_state: The latent input state.\n",
    "        :return: A data sample.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self, latent_state: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Train the generator from a given latent state.\n",
    "        \n",
    "        :param latent_state: The latent input state\n",
    "        :return: The loss symbol used for training\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        \n",
    "class InferenceNetwork(ABC):\n",
    "    \"\"\"\n",
    "    A network to infer distributions over latent states.\n",
    "\n",
    "    :param latent_variable_size: The dimensionality of the latent variable.\n",
    "    :param layer_sizes: Size of each layer in the network.\n",
    "    :param act_type: The activation after each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_variable_size: int, layer_sizes: List[int], act_type: str) -> None:\n",
    "        self.latent_var_size = latent_variable_size\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.act_type = act_type\n",
    "\n",
    "    def inference(self, data: mx.sym.Symbol) -> Tuple[mx.sym.Symbol, ...]:\n",
    "        \"\"\"\n",
    "        Infer the parameters of the distribution over latent values.\n",
    "\n",
    "        :param data: A data sample.\n",
    "        :return: The parameters of the distribution.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "class VAE(ABC):\n",
    "    \"\"\"\n",
    "    A variational autoencoding model (Kingma and Welling, 2013).\n",
    "\n",
    "    :param generator: A generator network that specifies the likelihood of the model.\n",
    "    :param inference_net: An inference network that specifies the distribution over latent values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, generator: Generator, inference_net: InferenceNetwork) -> None:\n",
    "        self.generator = generator\n",
    "        self.inference_net = inference_net\n",
    "\n",
    "    def train(self, data: mx.sym.Symbol, label: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Train the generator and inference network jointly by optimising the ELBO.\n",
    "\n",
    "        :param data: The training data.\n",
    "        :param label: Copy of the training data.\n",
    "        :return: A list of loss symbols.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def generate_reconstructions(self, data: mx.sym.Symbol, n: int) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Generate a number of reconstructions of input data points.\n",
    "\n",
    "        :param data: The input data.\n",
    "        :param n: Number of reconstructions per data point.\n",
    "        :return: The reconstructed data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def phantasize(self, n: int) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Generate data by randomly sampling from the prior.\n",
    "\n",
    "        :param n: Number of sampled data points.\n",
    "        :return: Randomly generated data points.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Let us start by implementing the generator. This is pretty much a standard neural network. The main point of this exercise is to get you comfortable with mxnet. Complete all the TODOs below. Before starting, check the activation\n",
    "functions available in mxnet [here](https://mxnet.incubator.apache.org/api/python/symbol.html#mxnet.symbol.Activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductOfBernoullisGenerator(Generator):\n",
    "    \"\"\"\n",
    "    A generator that produces binary vectors whose entries are independent Bernoulli draws.\n",
    "\n",
    "    :param data_dims: Dimensionality of the generated data.\n",
    "    :param layer_sizes: Size of each layer in the network.\n",
    "    :param act_type: The activation after each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dims: int, layer_sizes=List[int], act_type=str) -> None:\n",
    "        super().__init__(data_dims, layer_sizes, act_type)\n",
    "        # TODO choose the correct output activation for a Bernoulli variable. This should just be a string.\n",
    "        self.output_act = \"sigmoid\"\n",
    "\n",
    "    def _preactivation(self, latent_state: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Computes the pre-activation of the generator, i.e. the hidden state before the final output activation.\n",
    "\n",
    "        :param latent_state: The input latent state\n",
    "        :return: The pre-activation before output activation\n",
    "        \"\"\"\n",
    "        prev_out = None\n",
    "        for i, hidden in enumerate(self.layer_sizes):\n",
    "            fc_i = mx.sym.FullyConnected(data=latent_state, num_hidden=hidden, name=\"gen_fc_{}\".format(i))\n",
    "            act_i = mx.sym.Activation(data=fc_i, act_type=self.act_type, name=\"gen_act_{}\".format(i))\n",
    "            prev_out = act_i\n",
    "\n",
    "        # The output layer that gives pre_activations for multiple Bernoulli softmax between 0 and 1\n",
    "        fc_out = mx.sym.FullyConnected(data=prev_out, num_hidden=2 * self.data_dims, name=\"gen_fc_out\")\n",
    "\n",
    "        return fc_out\n",
    "    \n",
    "    def generate_sample(self, latent_state: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Generates a data sample by picking producing the maximally likely outcome. The stochasticity in the sampling\n",
    "        process comes from the latent_state.\n",
    "\n",
    "        :param latent_state: The input latent state.\n",
    "        :return: A vector of Bernoulli draws.\n",
    "        \"\"\"\n",
    "        act = mx.sym.Activation(data=self._generate(latent_state=latent_state), act_type=self.output_act,\n",
    "                                name=\"gen_act_out\")\n",
    "        act = mx.ndarray(mx.sym.split(data=act, num_outputs=self.data_dims))\n",
    "        out = mx.sym.maximum(data=act, axis=0)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def train(self, latent_state=mx.sym.Symbol, label=mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Train the generator from a given latent state\n",
    "\n",
    "        :param latent_state: The input latent state\n",
    "        :param label: A binary vector (same as input for inference module)\n",
    "        :return: The loss symbol used for training\n",
    "        \"\"\"\n",
    "        output = self._preactivation(latent_state=latent_state)\n",
    "        output = mx.sym.reshape(data=output, shape=(-1, 2, self.data_dims))\n",
    "        # We use a multi-ouput softmax. This is computes gradients for 784 independent softmaxes.\n",
    "        # Since a softmax of a 2-dim vector is a logistic function, we get the likelihood (and gradients)\n",
    "        # for 784 independent Bernoulli distributions as desired.\n",
    "        return mx.sym.SoftmaxOutput(data=output, label=label, multi_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "We now move on to the inference network. Recall that this network will return the parameters of a diagonal Gaussian. Thus, we need to return to vectors of the same size: a mean and a standard deviation vector. (Formally, the parameters of the Gaussian are the variances. However, from the derivation of the Gaussian reparametrisation we know that we\n",
    "need the standard deviations to generate a Gaussian random variable $z$ as transformation of a standard Gaussian variable $\\epsilon$.)\n",
    "\n",
    "**Hint:** In this exercise you will need to draw a random Gaussian sample (see [here](https://mxnet.incubator.apache.org/api/python/symbol.html#mxnet.symbol.random_normal)). The operator requires are\n",
    "shape whose first entry is the batch size. The batch size is not known to you during implementation, however.\n",
    "You can leave it underspecified by choosing $0$ as a value. When you combine the sampling operator with another\n",
    "operator immediately, mxnet will infer the correct the batch size for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianInferenceNetwork(InferenceNetwork):\n",
    "    \"\"\"\n",
    "    An inference network that predicts the parameters of a diagonal Gaussian and samples from that distribution.\n",
    "\n",
    "    :param latent_variable_size: The dimensionality of the latent variable.\n",
    "    :param layer_sizes: Size of each layer in the network.\n",
    "    :param act_type: The activation after each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_variable_size: int, layer_sizes: List[int], act_type: str):\n",
    "        super().__init__(latent_variable_size, layer_sizes, act_type)\n",
    "\n",
    "    def inference(self, data: mx.sym.Symbol) -> Tuple[mx.sym.Symbol, mx.sym.Symbol]:\n",
    "        \"\"\"\n",
    "        Infer the mean and standard deviation.\n",
    "\n",
    "        :param data: A data sample.\n",
    "        :return: The mean and standard deviation.\n",
    "        \"\"\"\n",
    "        # We choose to share the first layer between the networks that compute the standard deviations\n",
    "        # and means. This is a fairly standard design choice.\n",
    "        shared_layer = mx.sym.FullyConnected(data=data, num_hidden=self.layer_sizes[0], name=\"inf_joint_fc\")\n",
    "        shared_layer = mx.sym.Activation(data=shared_layer, act_type=self.act_type, name=\"inf_joint_act\")\n",
    "\n",
    "        prev_out = shared_layer\n",
    "        for i, size in enumerate(self.layer_sizes[1:]):\n",
    "            mean_fc_i = mx.sym.FullyConnected(data=prev_out, num_hidden=size, name=\"inf_mean_fc_{}\".format(i))\n",
    "            mean_act_i = mx.sym.Activation(data=mean_fc_i, act_type=self.act_type, name=\"inf_mean_act_{}\".format(i))\n",
    "            prev_out = mean_act_i\n",
    "        mean = mx.sym.FullyConnected(data=prev_out, num_hidden=self.latent_var_size, name=\"inf_mean_compute\")\n",
    "\n",
    "        prev_out = shared_layer\n",
    "        for i, size in enumerate(self.layer_sizes[1:]):\n",
    "            var_fc_i = mx.sym.FullyConnected(data=prev_out, num_hidden=size, name=\"rec_var_fc_{}\".format(i))\n",
    "            var_act_i = mx.sym.Activation(data=var_fc_i, act_type=self.act_type, name=\"rec_var_act_{}\".format(i))\n",
    "            prev_out = var_act_i\n",
    "        # soft-relu maps std onto non-negative real line\n",
    "        std = mx.sym.Activation(\n",
    "            mx.sym.FullyConnected(data=prev_out, num_hidden=self.latent_var_size, name=\"inf_var_compute\"),\n",
    "            act_type=\"softrelu\")\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "    def sample_latent_state(self, mean: mx.sym.Symbol, std: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Sample a latent Gaussian variable\n",
    "\n",
    "        :param mean: The mean of the Gaussian\n",
    "        :param std: The standard deviation of the Gaussian\n",
    "        :return: A Gaussian sample\n",
    "        \"\"\"\n",
    "        # TODO: This is where the magic happens! Draw a sample from the Gaussian using the Gaussian reparametrisation\n",
    "        # trick and return it.\n",
    "        return mean + std * mx.sym.random_normal(loc=0, scale=1, shape=(0, self.latent_var_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.a\n",
    "\n",
    "Finally, we will put it all together and build our VAE. Recall that the objective for the inference net contains a KL term. You will need to implement that KL-term. Once it is implemented, we can take advantage of autograd to get its gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonal_gaussian_kl(mean: mx.sym.Symbol, std: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "    var = std ** 2\n",
    "    return 0.5 * (mx.sym.sum(1 + mx.sym.log(var) - mean ** 2 - var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.b\n",
    "\n",
    "The only thing that is left to do is to implement VAE training. This is where you will have to define an additional loss with respect to the KL divergence for the inference net. In mxnet losses are defined using the [MakeLoss](https://mxnet.incubator.apache.org/api/python/symbol.html#mxnet.symbol.MakeLoss) symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianVAE(VAE):\n",
    "    \"\"\"\n",
    "    A VAE with Gaussian latent variables. It assumes a standard normal prior on the latent values.\n",
    "\n",
    "    :param generator: A generator network that specifies the likelihood of the model.\n",
    "    :param inference_net: An inference network that specifies the Gaussian over latent values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 generator: Generator,\n",
    "                 inference_net: GaussianInferenceNetwork,\n",
    "                 kl_divergence: Callable) -> None:\n",
    "        self.generator = generator\n",
    "        self.inference_net = inference_net\n",
    "        self.kl_divergence = kl_divergence\n",
    "\n",
    "    def train(self, data: mx.sym.Symbol, label: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Train the generator and inference network jointly by optimising the ELBO.\n",
    "\n",
    "        :param data: The training data.\n",
    "        :param label: Copy of the training data.\n",
    "        :return: A list of loss symbols.\n",
    "        \"\"\"\n",
    "        mean, std = self.inference_net.inference(data=data)\n",
    "        latent_state = self.inference_net.sample_latent_state(mean, std)\n",
    "        kl_loss = mx.sym.MakeLoss(self.kl_divergence(mean, std))\n",
    "        return mx.sym.Group([self.generator.train(latent_state=latent_state, label=label), kl_loss])\n",
    "\n",
    "    def generate_reconstructions(self, data: mx.sym.Symbol, n: int) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Generate a number of reconstructions of input data points.\n",
    "\n",
    "        :param data: The input data.\n",
    "        :param n: Number of reconstructions per data point.\n",
    "        :return: The reconstructed data.\n",
    "        \"\"\"\n",
    "        mean, std = self.inference_net.inference(data=data)\n",
    "        mean = mx.sym.tile(data=mean, reps=(n, 1))\n",
    "        std = mx.sym.tile(data=std, reps=(n, 1))\n",
    "        latent_state = self.sample_latent_state(mean, std, n)\n",
    "        return self.generator.generate_sample(latent_state=latent_state)\n",
    "\n",
    "    def phantasize(self, n: int) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Generate data by randomly sampling from the prior.\n",
    "\n",
    "        :param n: Number of sampled data points.\n",
    "        :return: Randomly generated data points.\n",
    "        \"\"\"\n",
    "        latent_state = mx.sym.random_normal(loc=0, scale=1, shape=(n, self.inference_net.latent_var_size))\n",
    "        return self.generator.generate_sample(latent_state=latent_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the VAE\n",
    "\n",
    "We have now fully specified a VAE. To get an impression of what its computation graph looks like, we can use mxnet's built-in visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_vae(latent_type: str,\n",
    "                  likelihood: str,\n",
    "                  generator_layer_sizes: List[int],\n",
    "                  infer_layer_sizes: List[int],\n",
    "                  latent_variable_size: int,\n",
    "                  data_dims: int,\n",
    "                  generator_act_type: str = \"tanh\",\n",
    "                  infer_act_type: str = \"tanh\") -> VAE:\n",
    "    \"\"\"\n",
    "    Construct a variational autoencoder\n",
    "\n",
    "    :param latent_type: Distribution of latent variable.\n",
    "    :param likelihood: Type of likelihood.\n",
    "    :param generator_layer_sizes: Sizes of generator hidden layers.\n",
    "    :param infer_layer_size: Sizes of inference network hidden layers.\n",
    "    :param latent_variable_size: Size of the latent variable.\n",
    "    :param data_dims: Dimensionality of the data.\n",
    "    :param generator_act_type: Activation function for generator hidden layers.\n",
    "    :param infer_act_type: Activation function for inference network hidden layers.\n",
    "    :return: A variational autoencoder.\n",
    "    \"\"\"\n",
    "    if likelihood == \"bernoulliProd\":\n",
    "        generator = ProductOfBernoullisGenerator(data_dims=data_dims, layer_sizes=generator_layer_sizes,\n",
    "                                                 act_type=generator_act_type)\n",
    "    else:\n",
    "        raise Exception(\"{} is an invalid likelihood type.\".format(likelihood))\n",
    "\n",
    "    if latent_type == \"gaussian\":\n",
    "        inference_net = GaussianInferenceNetwork(latent_variable_size=latent_variable_size,\n",
    "                                                 layer_sizes=infer_layer_sizes,\n",
    "                                                 act_type=infer_act_type)\n",
    "        return GaussianVAE(generator=generator, inference_net=inference_net, kl_divergence=diagonal_gaussian_kl)\n",
    "    else:\n",
    "        raise Exception(\"{} is an invalid latent variable type.\".format(latent_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = construct_vae(latent_type=\"gaussian\", likelihood=\"bernoulliProd\", generator_layer_sizes=[200,500],\n",
    "                   infer_layer_sizes=[500,200], latent_variable_size=100, data_dims=784, generator_act_type=\"tanh\",\n",
    "                   infer_act_type=\"tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mx.sym.Variable(\"data\")\n",
    "label = mx.sym.Variable(\"label\")\n",
    "mx.viz.plot_network(vae.train(data, label), title=\"bla.jpg\", save_format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
