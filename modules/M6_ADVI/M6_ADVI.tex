\documentclass[14pt,dvipsnames]{beamer}

\usetheme{Montpellier}
\usecolortheme{beaver}

\usepackage{physics}
\usepackage{amsmath, amssymb, ../../vimacros, hyperref, tikz}
\usetikzlibrary{positioning, fit, bayesnet, shapes.misc, patterns}
\usepackage[round]{natbib}
\usepackage{mathalfa}
\usepackage{cancel}

\beamertemplatenavigationsymbolsempty

\hypersetup{breaklinks=true, colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue}

\newcommand{\balert}[1]{\textcolor{blue}{#1}}
\newcommand{\galert}[1]{\textcolor{PineGreen}{#1}}

\pgfdeclarepatternformonly{stripes}
{\pgfpointorigin}{\pgfpoint{.4cm}{.4cm}}
{\pgfpoint{.4cm}{.4cm}}
{
	\pgfpathmoveto{\pgfpoint{0cm}{0cm}}
	\pgfpathlineto{\pgfpoint{.4cm}{.4cm}}
	\pgfpathlineto{\pgfpoint{.4cm}{.2cm}}
	\pgfpathlineto{\pgfpoint{.2cm}{0cm}}
	\pgfpathclose
	\pgfusepath{fill}
	\pgfpathmoveto{\pgfpoint{0cm}{0.2cm}}
	\pgfpathlineto{\pgfpoint{0cm}{.4cm}}
	\pgfpathlineto{\pgfpoint{0.2cm}{.4cm}}
	\pgfpathclose
	\pgfusepath{fill}
}

\newcommand\Wider[2][3em]{%
\makebox[\linewidth][c]{%
  \begin{minipage}{\dimexpr\textwidth+#1\relax}
  \raggedright#2
  \end{minipage}%
  }%
}

\makeatletter
\newenvironment{noheadline}{
    \setbeamertemplate{headline}{}
    \addtobeamertemplate{frametitle}{\vspace*{-0.9\baselineskip}}{}
}{}
\makeatother

\title{Automatic Differentiation Variational Inference}
\author{Philip Schulz and Wilker Aziz\\
\url{https://github.com/philschulz/VITutorial}}
\date{}

\setbeamertemplate{footline}[frame number]

\begin{document}

\begin{frame}
\maketitle
\end{frame}

\begin{frame}{What we know so far}
    \begin{itemize}
        \item DGMs: \pause probabilistic models parameterised by neural networks \pause
        \item Objective: \pause lowerbound on log-likelihood (ELBO) \pause
        \begin{itemize}
        		\item \alert{cannot be computed exactly} \\ \pause
        		\textcolor{blue}{we resort to Monte Carlo estimation} \pause		
	\end{itemize}
	\item \alert{But the MC estimator is not differentiable} \pause		        
	\begin{itemize}
       		\item Score function estimator: applicable to any model  \pause
		\item Reparameterised gradients\\
		so far seems applicable only to Gaussian variables
        \end{itemize}
    \end{itemize}
    
\end{frame}



\section{Multivariate calculus recap}

\begin{frame}{Multivariate calculus recap}

Let $x \in \mathbb R^K$ and let $\mathcal T: \mathbb R^K \to \mathbb R^K$ be differentiable and invertible
\begin{itemize}
	\item $y = \mathcal T(x)$
	\item $x = \inv{\mathcal T}(y)$
\end{itemize}

\end{frame}

\begin{frame}{Jacobian}

	The Jacobian matrix $\mathbf J = \jac{\mathcal T}{x} $ of  $\mathcal T$ \\
	~assessed at $x$ is the matrix of partial derivatives
	\begin{equation*}
		J_{ij} = \pdv{y_i}{x_j} 
	\end{equation*} 
	
	\pause
	Inverse function theorem
	\begin{equation*}
		\jac{\inv{\mathcal T}}{y} = \left( \jac{\mathcal T}{x} \right)^{-1}
	\end{equation*}
	
\end{frame}

\begin{frame}{Differential (or inifinitesimal)}

	The {\bf differential} $\dd x$ of $x$ \\
	~ refers to an \emph{infinitely small} change in $x$\\ \pause
	\vspace{10pt}

	We can relate the differential $\dd y$ of $y = \mathcal T(x)$ to $\dd x$ \pause
	\begin{itemize}
		\item Scalar case
		\begin{equation*}
			\dd y = \alert{\mathcal T'(x)} \dd x = \alert{\dv{y}{x}} \dd x = \alert{\dv{x}T(x)} \dd x
		\end{equation*}
		where \alert{$\dv*{y}{x}$} is the \emph{derivative} of $y$ wrt $x$ \pause
		\item Multivariate case
		\begin{equation*}
        			\begin{aligned}
			        \dd y = \alert{\djac{\mathcal T}{x}} \dd x % = \alert{\abs{\pdv{x}T(x)}} \dd x % in some texts people will find this notation
		        	\end{aligned}
	        	\end{equation*}
		the absolute value absorbs the orientation 
	\end{itemize}

\end{frame}

\begin{frame}{Integration by substitution}	
	We can integrate a function $g(x)$ \\
	~ by substituting $x = \inv{ \mathcal T}(y)$
	\begin{equation*}
	\begin{aligned}
		\int g(\balert{x}) \alert{\dd x} \pause &= \int g(\underbrace{\balert{\inv{\mathcal T}(y)}}_{x}) \underbrace{\alert{\djac{\inv{\mathcal T}}{y} \dd y}}_{\dd x} \\ \pause
	\end{aligned}
	\end{equation*}
	
	\vspace{-10pt}
	and similarly for a function $h(y)$
	\begin{equation*}
	\begin{aligned}
		\int h(\balert{y}) \alert{\dd y} \pause &= \int h(\balert{\mathcal T(x)}) \alert{\djac{\mathcal T}{x} \dd x}
	\end{aligned}
	\end{equation*} 

\end{frame}

\begin{frame}{Change of density}

Let $X$ take on values in $\mathbb R^K$ with density $f_X(x)$\\ \pause
~ and recall that $y = \mathcal T(x)$ and $x = \inv{\mathcal T}(y)$\\ \pause

~

Then $\mathcal T$ induces a density $f_Y(y)$ expressed as
\begin{equation*}
f_Y(y) = f_X(\inv{\mathcal T}(y)) \djac{\inv{\mathcal T}}{y}
\end{equation*} \pause
and then it follows that
\begin{equation*}
f_X(x) = f_Y(\mathcal T(x)) \djac{\mathcal T}{x}
\end{equation*}

	
\end{frame}

\section{Reparameterised gradients revisited}

\begin{frame}{Revisiting reparameterised gradients}
	Let $Z$ take on values in $\mathbb R^K$ with pdf $q(z|\lambda)$ \\
	
	~ \pause

	The idea is to count on a \emph{standardisation} procedure\\ \pause
	~ a transformation $\mathcal S_\lambda: \mathbb R^K \to \mathbb R^K$ such that \pause
	\begin{equation*}
	\begin{aligned}
	\mathcal S_\lambda(z) &\sim \pi(\epsilon) \\
 	\inv{\mathcal S}_\lambda(\epsilon) &\sim q(z|\lambda)
	\end{aligned}
	\end{equation*} 
	\begin{itemize}
		\item $\pi(\epsilon)$ does not depend on parameters $\lambda$\\
		we call it a \emph{standard} density \pause
		\item $\mathcal S_\lambda(z)$ absorbs dependency on $\lambda$ 
	\end{itemize}

\end{frame}

\begin{frame}{Reparameterised expectations}
	If we are interested in 
	\begin{equation*}
	\begin{aligned}
		&  \E[\alert{q(z|\lambda)}]{g(z)} \pause = \int \alert{q(z|\lambda)} g(z) \textcolor{blue}{\dd z} \\ \pause
		&= \int \underbrace{\alert{\pi(\mathcal S_\lambda(z)) \djac{S_\lambda}{z}}}_{\text{change of density}} g(z) \textcolor{blue}{\dd z} \\ \pause
		&= \int \alert{\pi(\epsilon)} \pause \underbrace{\alert{\djac{\inv{\mathcal S}_\lambda}{\epsilon}^{-1}}}_{\text{inv func theorem}} \pause g(\underbrace{\inv{\mathcal S}_\lambda(\epsilon)}_{z}) \pause \underbrace{\textcolor{blue}{\djac{\inv{\mathcal S}_\lambda}{\epsilon} \dd \epsilon}}_{\text{change of var}} \\ \pause
		&= \int \pi(\epsilon) g(\inv{\mathcal S}_\lambda(\epsilon))\dd \epsilon \pause = \E[\pi(\epsilon)]{g(\inv{\mathcal S}_\lambda(\epsilon)) }
	\end{aligned}
	\end{equation*}
\end{frame}

\begin{frame}{Reparameterised gradients}
	For optimisation, we need tractable gradients
	\begin{equation*}
		\begin{aligned}
			\pdv{\alert{\lambda}}  \E[\alert{q(z|\lambda)}]{g(z)} = \pdv{\alert{\lambda}} \E[\textcolor{blue}{\pi(\epsilon)}]{g(\inv{\mathcal S}_{\alert\lambda}(\epsilon)) }
		\end{aligned}
	\end{equation*} \pause
	since now the density does not depend on $\lambda$, we can obtain a gradient estimate
	\begin{equation*}
		\begin{aligned}
			&\pdv{\alert{\lambda}}  \E[\alert{q(z|\lambda)}]{g(z)} =  \E[\textcolor{blue}{\pi(\epsilon)}]{\pdv{\alert{\lambda}} g(\inv{\mathcal S}_{\alert\lambda}(\epsilon)) } \\ \pause
			&\overset{\text{MC}}{\approx}  \frac{1}{M} \sum_{\substack{i=1\\ \epsilon_i \sim \pi(\epsilon)}}^M \pdv{\alert{\lambda}} g(\inv{\mathcal S}_{\alert\lambda}(\epsilon_i)) 
		\end{aligned}
	\end{equation*}
\end{frame}

\begin{frame}{Standardisation functions}
	Location-scale family
	\begin{itemize}
		\item a family of distributions where for $F_X(x) = \Prob{X \le x}$ \\
		if $Y=a + b X$, then  $F_Y(y|a, b)=F_X(\frac{z-a}{b})$ \pause
		\item if we can draw from $f_X(x)$, we can draw from $f_Y(y|a,b)$ \pause
		\item the transformation absorbs the parameters $a, b$
		%\item $\frac{z - \mu}{\sigma}$ is the standardisation function \\
		%it's differentiable and invertible\\
		%$z  = \mu + \sigma \epsilon$		
	\end{itemize}
	
	\pause
	
	Examples: Gaussian, Laplace, Cauchy, Uniform
	
\end{frame}

\begin{frame}{Reparameterised gradients: Gaussian}
	We have  seen one case, namely,\\
	~ if $\epsilon \sim \mathcal N(0, I)$ and $Z \sim \mathcal N(\mu,\sigma^2)$\pause\\
	Then
	\begin{equation*}
	\begin{aligned}
		%\epsilon &\sim \mathcal N(0, 1) \\	\pause
		%Z &\sim \mathcal N(\mu, \sigma^2) \\ \pause
		Z &\sim \mu +  \sigma  \epsilon
	\end{aligned}
	\end{equation*}
	and
	\begin{equation*}
	\begin{aligned}
		&\pdv{\lambda} \E[\mathcal N(z|\mu, \sigma^2)]{ g(z) }\\ \pause
		&= \E[\mathcal N(0, I)]{\pdv{\lambda} g(z = \mu + \sigma  \epsilon)} \\ \pause
		&= \E[\mathcal N(0, I)]{\pdv{z} g(z = \mu + \sigma  \epsilon) \pdv{z}{\lambda}}
	\end{aligned}
	\end{equation*}
\end{frame}

\begin{frame}{Reparameterised gradients: Gaussian}
	Location
	\begin{equation*}
	\begin{aligned}
		\pdv{\mu} \E[\mathcal N(z|\mu, \sigma^2)]{ g(z) }
			&= \E[\mathcal N(0, I)]{\pdv{z} g(z = \mu + \sigma  \epsilon) \pdv{z}{\mu}} \\ \pause
		&= \E[\mathcal N(0, I)]{\pdv{z} g(z = \mu + \sigma  \epsilon)} \pause
	\end{aligned}
	\end{equation*}
	
	Scale
	\begin{equation*}
	\begin{aligned}
		\pdv{\sigma} \E[\mathcal N(z|\mu, \sigma^2)]{ g(z) } &= \E[\mathcal N(0, I)]{\pdv{z} g(z = \mu + \sigma  \epsilon) \pdv{z}{\sigma}} \\ \pause
		&= \E[\mathcal N(0, I)]{\pdv{z} g(z = \mu + \sigma  \epsilon)  \epsilon} 
	\end{aligned}
	\end{equation*}
	
\end{frame}

\begin{frame}{Standardisation functions (cont.)}
	Inverse cdf
	\begin{itemize}
		\item for univariate $Z$ with pdf $f_Z(z)$ and cdf $F_Z(z)$
		\begin{equation*}
		\begin{aligned}
			P \sim \mathcal U(0, 1) \qquad Z \sim \inv{F}_Z(P) 
		\end{aligned}		
		\end{equation*}
		where $\inv{F}_Z(p)$ is the \emph{quantile function}
	\end{itemize}
	
	~ \pause
	
	Gumbel distribution
	\begin{itemize}
		\item $f_Z(z|\mu, \beta) = \beta^{-1}\exp(-z -\exp(-z))$ 
		\item $F_Z(z|\mu, \beta) = \exp(-\exp(-\frac{z-\mu}{\beta}))$
		\item $\inv{F}_Z(p) = \mu - \beta \log( - \log p)$
	\end{itemize}
\end{frame}

\begin{frame}{Beyond}

	Many interesting densities are not location-scale families
	\begin{itemize}
		\item e.g. Beta, Gamma
	\end{itemize} \pause
	
	The inverse cdf of a multivariate rv is seldom known in closed-form
	\begin{itemize}
		\item Dirichlet, von Mises-Fisher
	\end{itemize}

\end{frame}

\section{ADVI}

\begin{frame}{Automatic Differentiation VI}
	Motivation
	\begin{itemize}
		\item many models have intractable posteriors\\
		their normalising constants (evidence) lacks analytic solutions \pause
		\item but many models are differentiable\\
		that's the main constraint for using NNs \pause
	\end{itemize}

	Reparameterised gradients are a step towards automatising VI for differentiable models \pause
	\begin{itemize}
		\item but not every model of interest employs rvs for which a standardisation function is known
	\end{itemize}
	
\end{frame}

\begin{frame}{Example}
	Suppose we have some ordinal data which we assume to be Poisson-distributed
	\begin{equation*}
		X|\lambda \sim \Poisson(\lambda)
	\end{equation*}
	and suppose we want to impose 
\end{frame}

\begin{frame}{Differentiable models}

	We focus on \emph{differentiable probability models}
	\begin{equation*}
		p(x,z) = p(x|z)p(z)
	\end{equation*}
	\pause
	\begin{itemize}
		\item members of this class have continuous latent variables $z$\\ \pause
		\item and the gradient $\grad_z \log p(x,z)$ is valid within the \emph{support} of the prior 
		$\supp(p(z)) = \{ z \in \mathbb R^K : p(z) > 0 \} \subseteq \mathbb R^K$
	\end{itemize}
	
\end{frame}

\begin{frame}{Why do we need differentiable models?}
	
	Recall the gradient of the ELBO
	\begin{equation*}
		\only<1>{\pdv{\lambda} \E[q(z; \lambda)]{\log p(x,z)}}\only<2->{\alert{\pdv{\lambda} \E[q(z; \lambda)]{\log p(x,z)}}} -  \only<1>{\pdv{\lambda} \Ent{q(z; \lambda)}}\only<2->{\textcolor{gray}{\pdv{\lambda} \Ent{q(z; \lambda)}}} 
	\end{equation*}
	
	\pause
	
	Reparameterisation requires $\pdv{}{z}$
	\uncover<3->{
	\begin{equation*}
	\begin{aligned}
		\pdv{\lambda} \E[q(z; \lambda)]{\log p(x,z)} \uncover<4->{= \E[\pi(\epsilon)]{\pdv{\lambda} \log p(x,z=\inv{\mathcal S}_\lambda(\epsilon))}} \\
		\uncover<5->{= \E[\pi(\epsilon)]{\alert{\pdv{z} \log p(x,z)} \pdv{\lambda} \inv{\mathcal S}_\lambda(\epsilon)}}
	\end{aligned}
	\end{equation*}
	}

\end{frame}

\begin{frame}{VI optimisation problem}
	Let's focus on the design and optimisation of the variational approximation
	\begin{equation*}
		\argmin_{\balert{q(z)}} \KL{\balert{q(z)}}{\alert{p(z|x)}}
	\end{equation*}

	\pause
	
	To automatise the search for a variational approximation $\balert{q(z)}$ we must ensure that\\
	 \begin{equation*}
	 	\supp(\balert{q(z)}) \subseteq \supp(\alert{p(z|x)})
	\end{equation*} \pause
	
	\vspace{-10pt}
	 \begin{itemize}
	 	\item otherwise KL is not defined\\
		$\KL{q}{p} = \E[q]{\log q} - \E[q]{\log p} = \infty$
	\end{itemize}	
	 
\end{frame}

\begin{frame}{Support matching constraint}

	So let's constrain $q(z)$ to a family $\mathcal Q$ whose support is included in the support of the \alert{posterior}
	 \begin{equation*}
		\argmin_{\balert{q(z)} \in \mathcal Q} \KL{\balert{q(z)}}{\alert{p(z|x)}}
	\end{equation*}
	where
	\begin{equation*}
	 	\mathcal Q = \{\balert{q(z)}: \supp(\balert{q(z)}) \subseteq \supp(\alert{p(z|x)}) \}
	\end{equation*}
	
	\pause
	
	\alert{But what is the support of $\alert{p(z|x)}$?} \pause
	 \begin{itemize}
		\item typically the same as the support of $\galert{p(z)}$\\ \pause
		as long as $p(x,z) > 0$ if $p(z) > 0$		
	 \end{itemize}
	 
\end{frame}

\begin{frame}{Parametric family}

	So let's constrain $q(z)$ to a family $\mathcal Q$ whose support is included in the support of the \galert{prior}
	 \begin{equation*}
		\argmin_{\balert{q(z)} \in \mathcal Q} \KL{\balert{q(z)}}{\alert{p(z|x)}}
	\end{equation*}
	where
	\begin{equation*}
	 	\mathcal Q = \{\balert{q(z; \lambda)}: \lambda \in \Lambda, \supp(\balert{q(z; \lambda)}) \subseteq \supp(\galert{p(z)})  \}
	\end{equation*}
	
	\vspace{-10pt} \pause
	 \begin{itemize}
	 	\item a parameter vector $\lambda$ picks out a member of the family
	\end{itemize}

\end{frame}

\begin{frame}{Constrained optimisation for the ELBO}

	We maximise the ELBO 
	\begin{equation*}
		\argmax_{\lambda \in \Lambda} \E[\balert{q(z; \lambda)}]{\log p(x, z)} + \Ent{\balert{q(z; \lambda)}}
	\end{equation*}
	\pause subject to
	 \begin{equation*}
		\mathcal Q = \{\balert{q(z; \lambda)}: \lambda \in \Lambda, \supp(\balert{q(z; \lambda)}) \subseteq \supp(\galert{p(z)})  \}
	\end{equation*}
	\pause
	
	\vspace{-10pt}
	There are really two constraints here\pause
	\begin{itemize}
		\item \galert{support matching constraint} \pause
		\item \alert{$\Lambda$ can be an intricate subset of $\mathbb R^D$}\\ \pause
		e.g. univariate Gaussian location lives in $\mathbb R$ but scale lives in $\mathbb R_{>0}$
	\end{itemize}

\end{frame}

\begin{frame}{Parameters in real coordinate space}
	It is easy to make sure parameters are unconstrained
	\vspace{-15pt}
	\begin{itemize}
		\item we just need to use an appropriate activation
	\end{itemize}
	
	Example 
	\begin{itemize}
		\item $Z \sim \mathcal N(\mu, \sigma)$ \\
		where $\mu \in \mathbb R^d$ and $\sigma \in \mathbb R^d_{>0}$ \pause
		\begin{itemize}
			\item $\mu = \lambda_\mu \in \mathbb R^d$ \pause
			\item $\sigma = \softplus(\lambda_\sigma)$ and $\lambda_\sigma \in \mathbb R^d$
		\end{itemize} \pause
		\item $Z \sim \vMF(\mu, \kappa)$ \\ 
		where $\mu \in \mathbb R^d$ with $\norm{\mu}_2 = 1$ and $\kappa \in \mathbb R_{\ge 0}$ \pause
		\begin{itemize}
			\item $\mu = \frac{\lambda_\mu}{\norm{\lambda_\mu}_2}$ for $\lambda_\mu \in \mathbb R^d$ \pause
			\item $\kappa = \softplus(\lambda_\kappa)$ and $\lambda_\kappa \in \mathbb R$
		\end{itemize}
		
	\end{itemize}
	
\end{frame}


\begin{frame}{Constrained optimisation for the ELBO}

	We maximise the ELBO 
	\begin{equation*}
		\argmax_{\lambda \in \galert{\mathbb R^D}} \E[\balert{q(z; \lambda)}]{\log p(x, z)} + \Ent{\balert{q(z; \lambda)}}
	\end{equation*}
	\pause 	subject to
	 \begin{equation*}
		\mathcal Q = \{\balert{q(z; \lambda)}: \lambda \in \galert{\mathbb R^D}, \supp(\balert{q(z; \lambda)}) \subseteq \alert{\supp(p(z))}  \}
	\end{equation*}
	
	\vspace{-10pt}
	There is one constraint left\pause
	\begin{itemize}
		\item \alert{support of $q(z; \lambda)$ depends on the choice of prior}\looseness=-1 \\
		\alert{and thus may be an intricate subset of $\mathbb R^K$}
	\end{itemize}

\end{frame}

\begin{frame}{ADVI}
	
	A gradient-based black-box VI procedure \pause
	\begin{enumerate}		
		\item \alert{Custom parameter space} \pause
			\begin{itemize}
				\item \galert{Appropriate transformations of unconstrained parameters!} \pause
			\end{itemize}
		\item \alert{Custom $\supp(p(z))$} \pause
			\begin{itemize}
				\item \galert{Express $z \in \supp(p(z)) \subseteq \mathbb R^K$ as a transformation of some unconstrained $\zeta \in \mathbb R^K$} \pause
				\item \galert{Pick a variational family over the entire real coordinate space} \pause
				\item \galert{And pick one for which a standardisation exists!} \pause
			\end{itemize}
		\item \alert{Intractable expectations} \pause
			\begin{itemize}
				\item \galert{Reparameterised Gradients!} 
			\end{itemize}
	\end{enumerate}
		
\end{frame}

\begin{frame}{Joint model in real coordinate space}

	Let's introduce an invertible and differentiable transformation
	\begin{equation*}
		\mathcal T: \supp(p(z)) \to \mathbb R^K
	\end{equation*} \pause
	 and define a transformed variable $\zeta \in \mathbb R^K$
	\begin{equation*}
		\zeta = \mathcal T(z)
 	\end{equation*} \pause
	
	\vspace{-10pt}	
	
	Recall that we have a joint density $p(x,z)$\\ \pause
	~ which we can use to construct $p(x, \zeta)$ \pause
	\begin{equation*}
		p(x, \zeta) = p(x, \inv{\mathcal T}(\zeta)) \djac{\inv{\mathcal T}}{\zeta}
	\end{equation*}
\end{frame}

\begin{frame}{VI in real coordinate space}
	We can design a posterior approximation whose support is $\mathbb R^K$\\ \pause
	
	\begin{equation*}
	\begin{aligned}
		q(\zeta; \lambda) \pause &= \underbrace{\galert{\prod_{k=1}^K} q(\zeta_{\galert{k}}; \lambda)}_{\text{mean field}} \pause
		&= \prod_{k=1}^K \galert{\mathcal N(\zeta_k|\mu_k, \sigma^2_k)} \\ 
	\end{aligned}
	\end{equation*}
	where
	\begin{itemize}
		\item $\mu_k = \lambda_{\mu_k}$ for $\lambda_{\mu_k} \in \mathbb R^K$
		\item $\sigma_k = \softplus(\lambda_{\sigma_k})$ for $\lambda_{\sigma_k} \in \mathbb R^K$
	\end{itemize}
	
\end{frame}


\begin{frame}{ELBO in real coordinate space}
\Wider[4em]{	
	\begin{equation*}
	\begin{aligned}
		&\log p(x) \pause = \log \int p(x,\alert{z}) \dd \alert{z}  \\ \pause
		&= \log \int p(x,\alert{\inv{\mathcal T}(\zeta)}) \alert{\djac{\inv{\mathcal T}}{\zeta} \dd \zeta}	\\	\pause
		&= \log \int \galert{q(\zeta)} \frac{p(x,\inv{\mathcal T}(\zeta)) \djac{\inv{\mathcal T}}{\zeta}}{\galert{q(\zeta)}} \dd \zeta \\ \pause
		&\overset{\text{JI}}{\alert{\ge}}  \int q(\zeta) \alert{\log} \frac{p(x,\inv{\mathcal T}(\zeta)) \djac{\inv{\mathcal T}}{\zeta}}{q(\zeta)} \dd \zeta \\ \pause
		&= \E[q(\zeta)]{\log p(x,\inv{\mathcal T}(\zeta)) + \log \djac{\inv{\mathcal T}}{\zeta}} + \Ent{q(\zeta)}
	\end{aligned}
	\end{equation*}
}
\end{frame}

\begin{frame}{Reparameterised ELBO}
	
	Recall that for Gaussians we have a standardisation procedure $\mathcal S_\lambda(z) \sim \mathcal N(\epsilon| 0, I)$
	
	\Wider[4em]{
	\begin{equation*}
	\begin{aligned}
		&\alert{\E[q(\zeta; \lambda)]{\log p(x, \inv{\mathcal T}(\zeta)) + \log \djac{\inv{\mathcal T}}{\zeta}}} + \galert{\Ent{q(\zeta; \lambda)}}  \\ \pause
		&= \E[\mathcal N(\epsilon|0, I)]{\log p(x, \underbrace{\inv{\mathcal T}(\overbrace{\mathcal S_\lambda(\epsilon)}^{\zeta})}_{z}) + \log \djac{\inv{\mathcal T}}{\mathcal S_\lambda(\epsilon)} } \\
		&+ \Ent{q(\zeta; \lambda)}
	\end{aligned}
	\end{equation*}
	}

\end{frame}

\begin{frame}{Gradient estimate}

	
	\begin{small}
	For $\epsilon_i \sim \mathcal N(0, I)$
	
	\begin{equation*}
		\begin{aligned}
		\pdv{\lambda}\ELBO(\lambda) \pause \overset{\text{MC}}{\approx}& \frac{1}{M} \sum_{i=1}^M \pdv{\lambda} \log \underbrace{p(x|\inv{\mathcal T}(\mathcal S_\lambda(\epsilon_i)))}_{\text{likelihood}} \\ \pause
		&\phantom{\frac{1}{M} \sum}+ \pdv{\lambda} \log \underbrace{p(\inv{\mathcal T}(\mathcal S_\lambda(\epsilon_i)))}_{\text{prior}} \\ \pause
		&\phantom{\frac{1}{M} \sum}+ \pdv{\lambda} \log \underbrace{\djac{\inv{\mathcal T}}{\mathcal S_\lambda(\epsilon_i)}}_{\text{change of space}} \\ \pause
		&+ \pdv{\lambda} \underbrace{\Ent{q(\zeta; \lambda)}}_{\text{analaytic}}
		\end{aligned}
	\end{equation*}
	
	\end{small}

\end{frame}

\begin{frame}{Relevance}

	Many software packages know how to transform the support of various distributions
	\begin{itemize}
		\item Stan
		\item Tensorflow \texttt{tf.probability}
		\item Pytorch \texttt{torch.distributions}
	\end{itemize}
\end{frame}

\section{Example}

\begin{frame}{LDA}
\end{frame}

\begin{frame}{Wait... no deep learning?}
\end{frame}



\begin{frame}[allowframebreaks]
\bibliographystyle{plainnat}
\bibliography{../../VI}
\end{frame}


\end{document}